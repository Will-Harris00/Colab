{"cells":[{"cell_type":"markdown","metadata":{"id":"Dto-uOuWSZIE"},"source":["# Mask R-CNN - Train cell nucleus Dataset\n","\n","\n","This notebook shows how to train Mask R-CNN implemented on coco on your own dataset. I trained the model to segment cell nucleus objects in an image. You'd need a GPU, because the network backbone is a Resnet101, which would be too slow to train on a CPU.The code is execuatble on google colaboratory GPU.  On google colab you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6523,"status":"ok","timestamp":1652205154410,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"1Lx5py8BxYw9","outputId":"3c25b723-9b7c-4af2-816a-9f25271cee20"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000001?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000001?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000001?line=3'>4</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mpip\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minstall wget\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%pip install wget\n","%pip install pycocotools"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":688,"status":"ok","timestamp":1652205155094,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"EMwapxaFSZIH","outputId":"495ef932-9a53-4ebc-fe8d-b0484fdd457d"},"outputs":[{"ename":"RuntimeError","evalue":"module compiled against API version 0xe but this version of numpy is 0xd","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"]},{"ename":"ImportError","evalue":"numpy.core.multiarray failed to import","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32m/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000002?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000002?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000002?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000002?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000002?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n","File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/cv2/__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///Users/wjhar/miniforge3/lib/python3.9/site-packages/cv2/__init__.py?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimportlib\u001b[39;00m\n\u001b[1;32m      <a href='file:///Users/wjhar/miniforge3/lib/python3.9/site-packages/cv2/__init__.py?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m----> <a href='file:///Users/wjhar/miniforge3/lib/python3.9/site-packages/cv2/__init__.py?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcv2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///Users/wjhar/miniforge3/lib/python3.9/site-packages/cv2/__init__.py?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcv2\u001b[39;00m \u001b[39mimport\u001b[39;00m _registerMatType\n\u001b[1;32m     <a href='file:///Users/wjhar/miniforge3/lib/python3.9/site-packages/cv2/__init__.py?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m mat_wrapper\n","\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"]}],"source":["import os\n","import sys\n","import random\n","import math\n","import re\n","import time\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import json\n","import pandas as pd\n","import wget\n","from zipfile import ZipFile\n","import tensorflow as tf\n","import datetime\n","\n","from pycocotools.coco import COCO\n","import skimage.io as io\n","from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# from keras.callbacks import Callback\n","\n","from keras.layers import Layer\n","\n","from skimage.io import imread, imshow, imread_collection, concatenate_images\n","from skimage.transform import resize\n","\n","# download dataset from github or mount from google drive\n","# https://github.com/chaozhong2010/HRSID\n","\n","# Directory of images to run detection on\n","DATA_DIR = os.path.join('drive/MyDrive/HRSID/HRSID_png/')\n","DATA_TYPE = 'train_test2017'\n","annotations_file = '{}/annotations/{}.json'.format(DATA_DIR, DATA_TYPE)\n","IMAGE_DIR = '{}/images/'.format(DATA_DIR)\n","\n","dpi = 192 # monitor resolution\n","\n","# Initialise the COCO api for instance annotations\n","coco = COCO(annotations_file)\n","\n","# Load the categories in a variable\n","category_ids = coco.getCatIds(catNms=['ship'])\n","categories = coco.loadCats(category_ids)\n","\n","print(categories)\n","\n","# Get all images containing the above Category IDs\n","img_ids = coco.getImgIds(catIds=category_ids)\n","DATASET_SIZE = len(img_ids)\n","print(\"\\nNumber of images containing ships in the %s annotations file:\" % DATA_TYPE, DATASET_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1652205155094,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"ov1QUWAq0J89"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000003?line=11'>12</a>\u001b[0m   \u001b[39mprint\u001b[39m(src)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000003?line=12'>13</a>\u001b[0m   \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mutils.py\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mwrite(src)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000003?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000003?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m'\u001b[39m\u001b[39mmodel.py\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/HRSID.ipynb#ch0000003?line=16'>17</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPlease upload the model.py to your google colab\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/utils.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/utils.py?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/utils.py?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/utils.py?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/utils.py?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/wjhar/Library/CloudStorage/OneDrive-Personal/Documents/Experimental/colab/utils.py?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mskimage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolor\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}],"source":["if not os.path.exists('config.py'):\n","  print(\"Please upload the config.py to your google colab\")\n","  from google.colab import files\n","  src = list(files.upload().values())[0]\n","  open('config.py','wb').write(src)\n","import config\n","\n","if not os.path.exists('utils.py'):\n","  print(\"Please upload the utils.py to your google colab\")\n","  from google.colab import files\n","  src = list(files.upload().values())[0]\n","  print(src)\n","  open('utils.py','wb').write(src)\n","import utils\n","\n","if not os.path.exists('model.py'):\n","  print(\"Please upload the model.py to your google colab\")\n","  from google.colab import files\n","  src = list(files.upload().values())[0]\n","  open('model.py','wb').write(src)\n","import model\n","\n","if not os.path.exists('visualize.py'):\n","  print(\"Please upload the visualize.py to your google colab\")\n","  from google.colab import files\n","  src = list(files.upload().values())[0]\n","  open('visualize.py','wb').write(src)\n","import visualize\n","\n","from config import Config\n","import utils\n","import model as modellib\n","import visualize\n","from model import log"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1652205155094,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"n5qEMfD1z_u_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","%matplotlib inline\n","\n","# Root directory of the project\n","ROOT_DIR = os.getcwd()\n","\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n","\n","# Local path to trained weights file\n","COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","\n","# Download COCO trained weights from Releases if needed\n","if not os.path.exists(COCO_MODEL_PATH):\n","    utils.download_trained_weights(COCO_MODEL_PATH)\n","\n","# Directory to save logs and model checkpoints, if not provided\n","# through the command line argument --logs\n","DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"]},{"cell_type":"markdown","metadata":{"id":"frlv78boCmiU"},"source":["## Calculate MAP, precision, recall for each image"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1652205155094,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"C9LrEDfBCHJq"},"outputs":[],"source":["# class EvalImage():\n","#   def __init__(self,dataset,model,cfg):\n","#     self.dataset = dataset\n","#     self.model   = model\n","#     self.cfg     = cfg\n","\n"," \n"," \n","#   def evaluate_model(self , len = 50):\n","#     APs = list()\n","#     precisions_dict = {}\n","#     recall_dict     = {}\n","#     for index,image_id in enumerate(self.dataset.image_ids):\n","#       if(index > len):\n","#          break; \n","#       # load image, bounding boxes and masks for the image id\n","#       image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(self.dataset, self.cfg,image_id, use_mini_mask=False)\n","#       # convert pixel values (e.g. center)\n","#       #scaled_image = modellib.mold_image(image, self.cfg)\n","#       # convert image into one sample\n","#       sample = np.expand_dims(image, 0)\n","#      # print(len(image))\n","#       # make prediction\n","#       yhat = self.model.detect(sample, verbose=1)\n","#       # extract results for first sample\n","#       r = yhat[0]\n","#       # calculate statistics, including AP\n","#       AP, precisions, recalls, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n","#       precisions_dict[image_id] = np.mean(precisions)\n","#       recall_dict[image_id] = np.mean(recalls)\n","#       # store\n","#       APs.append(AP)\n","\n","#     # calculate the mean AP across all images\n","#     mAP = np.mean(APs)\n","#     return mAP,precisions_dict,recall_dict"]},{"cell_type":"markdown","metadata":{"id":"pXOPFKUxCvMb"},"source":["## Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1652205155095,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"jBONWUhASZIO","outputId":"9279905d-4f82-4c11-e877-1880757104fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Configurations:\n","BACKBONE                       resnet101\n","BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n","BATCH_SIZE                     1\n","BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n","COMPUTE_BACKBONE_SHAPE         None\n","DETECTION_MAX_INSTANCES        100\n","DETECTION_MIN_CONFIDENCE       0.7\n","DETECTION_NMS_THRESHOLD        0.3\n","FPN_CLASSIF_FC_LAYERS_SIZE     1024\n","GPU_COUNT                      1\n","GRADIENT_CLIP_NORM             5.0\n","IMAGES_PER_GPU                 1\n","IMAGE_CHANNEL_COUNT            3\n","IMAGE_MAX_DIM                  896\n","IMAGE_META_SIZE                14\n","IMAGE_MIN_DIM                  896\n","IMAGE_MIN_SCALE                0\n","IMAGE_RESIZE_MODE              square\n","IMAGE_SHAPE                    [896 896   3]\n","LEARNING_MOMENTUM              0.9\n","LEARNING_RATE                  0.001\n","LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n","MASK_POOL_SIZE                 14\n","MASK_SHAPE                     [28, 28]\n","MAX_GT_INSTANCES               100\n","MEAN_PIXEL                     [123.7 116.8 103.9]\n","MINI_MASK_SHAPE                (896, 896)\n","NAME                           shapes\n","NUM_CLASSES                    2\n","POOL_SIZE                      7\n","POST_NMS_ROIS_INFERENCE        1000\n","POST_NMS_ROIS_TRAINING         2000\n","PRE_NMS_LIMIT                  6000\n","ROI_POSITIVE_RATIO             0.33\n","RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n","RPN_ANCHOR_SCALES              (8, 16, 64, 128, 256)\n","RPN_ANCHOR_STRIDE              1\n","RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n","RPN_NMS_THRESHOLD              0.7\n","RPN_TRAIN_ANCHORS_PER_IMAGE    256\n","STEPS_PER_EPOCH                200\n","TOP_DOWN_PYRAMID_SIZE          256\n","TRAIN_BN                       False\n","TRAIN_ROIS_PER_IMAGE           800\n","USE_MINI_MASK                  False\n","USE_RPN_ROIS                   True\n","VALIDATION_STEPS               50\n","WEIGHT_DECAY                   0.0001\n","\n","\n"]}],"source":["class ShapesConfig(Config):\n","    \"\"\"Configuration for training on the dataset.\n","    Derives from the base Config class and overrides values specific\n","    to the dataset.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"shapes\"\n","\n","    # Train on 1 GPU and 1 images per GPU. We can put multiple images on each\n","    # GPU. Batch size is (GPUs * images/GPU).\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 1  # background + nucleus\n","\n","    # Use small images for faster training. Set the limits of the small side\n","    # the large side, and that determines the image shape.\n","    IMAGE_MIN_DIM = 832\n","    IMAGE_MAX_DIM = 832\n","    IMAGE_SHAPE=[832, 832, 3]\n","\n","    # Use smaller anchors because our image and objects are small\n","    RPN_ANCHOR_SCALES = (8, 16, 64, 128, 256)  # anchor side in pixels\n","\n","    # Aim to allow ROI sampling to pick 33% positive ROIs.\n","    TRAIN_ROIS_PER_IMAGE = 800\n","\n","    # set number of epoch\n","    STEPS_PER_EPOCH = 200\n","\n","    # set validation steps \n","    VALIDATION_STEPS = 50\n","\n","    # The height and width are those of the image unless use_mini_mask is True,\n","    # in which case they are defined in MINI_MASK_SHAPE.\n","    # If enabled, resizes instance masks to a smaller size to reduce\n","    # memory load. Recommended when using high-resolution images.\n","    USE_MINI_MASK = False\n","    MINI_MASK_SHAPE = (IMAGE_SHAPE[0], IMAGE_SHAPE[1])  # (height, width) of the mini-mask\n","\n","    \n","config = ShapesConfig()\n","config.display()"]},{"cell_type":"markdown","metadata":{"id":"7dhvNUELSZIS"},"source":["## Notebook Preferences"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652205155095,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"sj3zh7wMSZIW"},"outputs":[],"source":["def get_ax(rows=1, cols=1, size=8):\n","    \"\"\"Return a Matplotlib Axes array to be used in\n","    all visualizations in the notebook. Provide a\n","    central point to control graph sizes.\n","    \n","    Change the default size attribute to control the size\n","    of rendered images\n","    \"\"\"\n","    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n","    return ax"]},{"cell_type":"markdown","metadata":{"id":"sjEu-7I1SZIY"},"source":["## Dataset\n","\n","Create a synthetic dataset\n","\n","Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n","\n","* load_image()\n","* load_mask()\n","* image_reference()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":394,"status":"ok","timestamp":1652205155487,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"7jWV6Gjz6378"},"outputs":[],"source":["class ShipsDataset(utils.Dataset):\n","    def load_coco(self, dataset_dir, subset, class_ids=None,\n","                  class_map=None, return_coco=False, auto_download=False):\n","        \"\"\"Load a subset of the COCO dataset.\n","        dataset_dir: The root directory of the COCO dataset.\n","        subset: What to load (train, val, minival, valminusminival)\n","        year: What dataset year to load (2014, 2017) as a string, not an integer\n","        class_ids: If provided, only loads images that have the given classes.\n","        class_map: TODO: Not implemented yet. Supports maping classes from\n","            different datasets to the same class ID.\n","        return_coco: If True, returns the COCO object.\n","        auto_download: Automatically download and unzip MS-COCO images and annotations\n","        \"\"\"\n","\n","        if auto_download is True:\n","            self.auto_download(dataset_dir, subset)\n","        print(dataset_dir)\n","        print(subset)\n","        coco = COCO(annotations_file)\n","        if subset == \"minival\" or subset == \"valminusminival\":\n","            subset = \"val\"\n","        image_dir = IMAGE_DIR\n","\n","        # Load all classes or a subset?\n","        if not class_ids:\n","            # All classes\n","            class_ids = sorted(coco.getCatIds())\n","\n","        # All images or a subset?\n","        if class_ids:\n","            image_ids = []\n","            for id in class_ids:\n","                image_ids.extend(list(coco.getImgIds(catIds=[id])))\n","            # Remove duplicates\n","            image_ids = list(set(image_ids))\n","        else:\n","            # All images\n","            image_ids = list(coco.imgs.keys())\n","\n","        # Add classes\n","        for i in class_ids:\n","            self.add_class(\"coco\", i, coco.loadCats(i)[0][\"name\"])\n","\n","        # Add images\n","        print(image_dir)\n","        coco.imgs[0]['file_name']\n","        for i in image_ids:\n","            self.add_image(\n","                \"coco\", image_id=i,\n","                path=os.path.join(image_dir, coco.imgs[i]['file_name']),\n","                file_name=coco.imgs[i]['file_name'],\n","                width=config.IMAGE_SHAPE[0],\n","                height=config.IMAGE_SHAPE[1],\n","                annotations=coco.loadAnns(coco.getAnnIds(\n","                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n","        if return_coco:\n","            return coco\n","\n","    def auto_download(self, dataDir, dataType, dataYear):\n","        \"\"\"Download the COCO dataset/annotations if requested.\n","        dataDir: The root directory of the COCO dataset.\n","        dataType: What to load (train, val, minival, valminusminival)\n","        dataYear: What dataset year to load (2014, 2017) as a string, not an integer\n","        Note:\n","            For 2014, use \"train\", \"val\", \"minival\", or \"valminusminival\"\n","            For 2017, only \"train\" and \"val\" annotations are available\n","        \"\"\"\n","\n","        # Setup paths and file names\n","        if dataType == \"minival\" or dataType == \"valminusminival\":\n","            imgDir = \"{}/{}{}\".format(dataDir, \"val\", dataYear)\n","            imgZipFile = \"{}/{}{}.zip\".format(dataDir, \"val\", dataYear)\n","            imgURL = \"http://images.cocodataset.org/zips/{}{}.zip\".format(\"val\", dataYear)\n","        else:\n","            imgDir = \"{}/{}{}\".format(dataDir, dataType, dataYear)\n","            imgZipFile = \"{}/{}{}.zip\".format(dataDir, dataType, dataYear)\n","            imgURL = \"http://images.cocodataset.org/zips/{}{}.zip\".format(dataType, dataYear)\n","        # print(\"Image paths:\"); print(imgDir); print(imgZipFile); print(imgURL)\n","\n","        # Create main folder if it doesn't exist yet\n","        if not os.path.exists(dataDir):\n","            os.makedirs(dataDir)\n","\n","        # Download images if not available locally\n","        if not os.path.exists(imgDir):\n","            os.makedirs(imgDir)\n","            print(\"Downloading images to \" + imgZipFile + \" ...\")\n","            with urllib.request.urlopen(imgURL) as resp, open(imgZipFile, 'wb') as out:\n","                shutil.copyfileobj(resp, out)\n","            print(\"... done downloading.\")\n","            print(\"Unzipping \" + imgZipFile)\n","            with zipfile.ZipFile(imgZipFile, \"r\") as zip_ref:\n","                zip_ref.extractall(dataDir)\n","            print(\"... done unzipping\")\n","        print(\"Will use images in \" + imgDir)\n","\n","        # Setup annotations data paths\n","        annDir = \"{}/annotations\".format(dataDir)\n","        if dataType == \"minival\":\n","            annZipFile = \"{}/instances_minival2014.json.zip\".format(dataDir)\n","            annFile = \"{}/instances_minival2014.json\".format(annDir)\n","            annURL = \"https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?dl=0\"\n","            unZipDir = annDir\n","        elif dataType == \"valminusminival\":\n","            annZipFile = \"{}/instances_valminusminival2014.json.zip\".format(dataDir)\n","            annFile = \"{}/instances_valminusminival2014.json\".format(annDir)\n","            annURL = \"https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?dl=0\"\n","            unZipDir = annDir\n","        else:\n","            annZipFile = \"{}/annotations_trainval{}.zip\".format(dataDir, dataYear)\n","            annFile = \"{}/instances_{}{}.json\".format(annDir, dataType, dataYear)\n","            annURL = \"http://images.cocodataset.org/annotations/annotations_trainval{}.zip\".format(dataYear)\n","            unZipDir = dataDir\n","        # print(\"Annotations paths:\"); print(annDir); print(annFile); print(annZipFile); print(annURL)\n","\n","        # Download annotations if not available locally\n","        if not os.path.exists(annDir):\n","            os.makedirs(annDir)\n","        if not os.path.exists(annFile):\n","            if not os.path.exists(annZipFile):\n","                print(\"Downloading zipped annotations to \" + annZipFile + \" ...\")\n","                with urllib.request.urlopen(annURL) as resp, open(annZipFile, 'wb') as out:\n","                    shutil.copyfileobj(resp, out)\n","                print(\"... done downloading.\")\n","            print(\"Unzipping \" + annZipFile)\n","            with zipfile.ZipFile(annZipFile, \"r\") as zip_ref:\n","                zip_ref.extractall(unZipDir)\n","            print(\"... done unzipping\")\n","        print(\"Will use annotations in \" + annFile)\n","\n","    def load_mask(self, image_id):\n","        \"\"\"Load instance masks for the given image.\n","        Different datasets use different ways to store masks. This\n","        function converts the different mask format to one format\n","        in the form of a bitmap [height, width, instances].\n","        Returns:\n","        masks: A bool array of shape [height, width, instance count] with\n","            one mask per instance.\n","        class_ids: a 1D array of class IDs of the instance masks.\n","        \"\"\"\n","        # If not a COCO image, delegate to parent class.\n","        image_info = self.image_info[image_id]\n","        if image_info[\"source\"] != \"coco\":\n","            return super(CocoDataset, self).load_mask(image_id)\n","\n","        instance_masks = []\n","        class_ids = []\n","        # print(self.image_info[image_id][\"annotations\"])\n","        annotations = self.image_info[image_id][\"annotations\"]\n","        # Build mask of shape [height, width, instance_count] and list\n","        # of class IDs that correspond to each channel of the mask.\n","        for annotation in annotations:\n","            class_id = self.map_source_class_id(\n","                \"coco.{}\".format(annotation['category_id']))\n","            if class_id:\n","                # print(config.MINI_MASK_SHAPE)\n","                empty_mask = np.zeros((800,800))\n","                # print(empty_mask.shape)\n","                m = np.maximum(coco.annToMask(annotation), empty_mask)\n","                # # Some objects are so small that they're less than 1 pixel area\n","                # and end up rounded out. Skip those objects.\n","                # if m.max() < 1:\n","                #     continue\n","                # Is it a crowd? If so, use a negative class ID.\n","                # if annotation['iscrowd']:\n","                    # Use negative class ID for crowds\n","                    # class_id *= -1\n","                    # For crowd masks, annToMask() sometimes returns a mask\n","                    # smaller than the given dimensions. If so, resize it.\n","                if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n","                    m = resize(m, (832, 832)) # resize the ground truth to the new image size\n","                instance_masks.append(m)\n","                class_ids.append(class_id)\n","\n","        # Pack instance masks into an array\n","        if class_ids:\n","            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n","            class_ids = np.array(class_ids, dtype=np.int32)\n","            return mask, class_ids\n","        else:\n","            # Call super class to return an empty mask\n","            return super(CocoDataset, self).load_mask(image_id)\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"coco\":\n","            return \"http://cocodataset.org/#explore?id={}\".format(info[\"id\"])\n","        else:\n","            super(CocoDataset, self).image_reference(image_id)\n","\n","    # # The following two functions are from pycocotools with a few changes.\n","\n","    # def annToRLE(self, ann, height, width):\n","    #     \"\"\"\n","    #     Convert annotation which can be polygons, uncompressed RLE to RLE.\n","    #     :return: binary mask (numpy 2D array)\n","    #     \"\"\"\n","    #     segm = ann['segmentation']\n","    #     if isinstance(segm, list):\n","    #         # polygon -- a single object might consist of multiple parts\n","    #         # we merge all parts into one mask rle code\n","    #         rles = maskUtils.frPyObjects(segm, height, width)\n","    #         rle = maskUtils.merge(rles)\n","    #     elif isinstance(segm['counts'], list):\n","    #         # uncompressed RLE\n","    #         rle = maskUtils.frPyObjects(segm, height, width)\n","    #     else:\n","    #         # rle\n","    #         rle = ann['segmentation']\n","    #     return rle\n","\n","    # def annToMask(self, ann, height, width):\n","    #     \"\"\"\n","    #     Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n","    #     :return: binary mask (numpy 2D array)\n","    #     \"\"\"\n","    #     rle = self.annToRLE(ann, height, width)\n","    #     m = maskUtils.decode(rle)\n","    #     return m\n","    \n","    def load_image(self, image_id):\n","      image_info = self.image_info[image_id]\n","      path = image_info.get('path')\n","      # print(image_info)\n","      # print(path)\n","      img = imread(path)[:,:,:3]\n","      img = resize(img, (config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1]), mode='constant', preserve_range=True)\n","    \n","      return img, image_info\n","\n","      \n","  # def load_image(self, image_id):\n","  #     \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n","  #     \"\"\"\n","  #     # Load image\n","  #     image = skimage.io.imread(self.image_info[image_id]['path'])\n","  #     # If grayscale. Convert to RGB for consistency.\n","  #     if image.ndim != 3:\n","  #         image = skimage.color.gray2rgb(image)\n","  #     # If has an alpha channel, remove it for consistency\n","  #     if image.shape[-1] == 4:\n","  #         image = image[..., :3]\n","  #     return image\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652205155487,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"YqkR3-OHSZIY"},"outputs":[],"source":["# class ShapesDataset(utils.Dataset):\n","    \n","#     def load_shapes(self, mode):\n","        \n","#         # Add classes\n","#         self.add_class(\"shapes\", 1, \"nucleus\")\n","        \n","\n","#         if mode == \"train\":  \n","#             for n, id_ in enumerate(train_ids):\n","#                 if n < int(len(train_ids) * 0.9):\n","#                     path = TRAIN_PATH + id_\n","#                     img_path = path + '/images/'\n","#                     self.add_image(\"shapes\", image_id=id_, path=img_path)\n","              \n","#         if mode == \"val\":   \n","#             for n, id_ in enumerate(train_ids):\n","#                 if n >= int(len(train_ids) * 0.9):\n","#                     path = TRAIN_PATH + id_\n","#                     img_path = path + '/images/'\n","#                     self.add_image(\"shapes\", image_id=id_, path=img_path)      \n","\n","#     def load_image(self, image_id):\n","        \n","#         info = self.image_info[image_id]\n","#         info = info.get(\"id\")\n","       \n","#         path = TRAIN_PATH + info\n","#         img = imread(path + '/images/' + info + '.png')[:,:,:3]\n","#         img = resize(img, (config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1]), mode='constant', preserve_range=True)\n","       \n","#         return img\n","\n","#     def image_reference(self, image_id):\n","#         \"\"\"Return the shapes data of the image.\"\"\"\n","#         info = self.image_info[image_id]\n","#         if info[\"source\"] == \"shapes\":\n","#             return info[\"shapes\"]\n","#         else:\n","#             super(self.__class__).image_reference(self, image_id)\n","\n","#     def load_mask(self, image_id):\n","#         \"\"\"Generate instance masks for shapes of the given image ID.\n","#         \"\"\"\n","        \n","#         info = self.image_info[image_id]\n","#         info = info.get(\"id\")\n","#         path = TRAIN_PATH + info\n","#         number_of_masks = len(next(os.walk(path + '/masks/'))[2])\n","#         mask = np.zeros([config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], number_of_masks], dtype=np.uint8)\n","#         iterator = 0\n","#         for mask_file in next(os.walk(path + '/masks/'))[2]:\n","#             mask_ = imread(path + '/masks/' + mask_file)\n","#             mask_ = resize(mask_, (config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1]), mode='constant', preserve_range=True)\n","#             mask[:, :, iterator] = mask_\n","#             iterator += 1\n","#             # Handle occlusions\n","#         occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n","#         for i in range(number_of_masks-2, -1, -1):\n","#             mask[:, :, i] = mask[:, :, i] * occlusion\n","#             occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n","            \n","#         # Map class names to class IDs.\n","#         class_ids = np.ones((number_of_masks,), dtype=int)\n","        \n","#         return mask, class_ids.astype(np.int32)\n","         \n","       \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1969,"status":"ok","timestamp":1652205157454,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"wAhNtW_TSZIb","outputId":"b4b232e5-46d3-4449-fee4-085a6634c5a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["drive/MyDrive/HRSID/HRSID_png/\n","train_test2017\n","loading annotations into memory...\n","Done (t=0.95s)\n","creating index...\n","index created!\n","drive/MyDrive/HRSID/HRSID_png//images/\n","drive/MyDrive/HRSID/HRSID_png/\n","train_test2017\n","loading annotations into memory...\n","Done (t=0.56s)\n","creating index...\n","index created!\n","drive/MyDrive/HRSID/HRSID_png//images/\n"]}],"source":["# Training dataset\n","dataset_train = ShipsDataset()\n","dataset_train.load_coco(DATA_DIR, DATA_TYPE)\n","dataset_train.prepare()\n","\n","# Validation dataset\n","dataset_val = ShipsDataset()\n","dataset_val.load_coco(DATA_DIR, DATA_TYPE)\n","dataset_val.prepare()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1652205157454,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"80U5OZQ8JuN9"},"outputs":[],"source":["# eval = EvalImage(dataset_train,model,config)\n","# # eval.evaluate_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":1700,"status":"error","timestamp":1652205159150,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"yAizzQC5SZIf","outputId":"c9446568-994c-43e9-83e6-94ee72706f0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["drive/MyDrive/HRSID/HRSID_png//images/P0123_4800_5600_5400_6200.png\n","{'id': 5140, 'source': 'coco', 'path': 'drive/MyDrive/HRSID/HRSID_png//images/P0123_4800_5600_5400_6200.png', 'width': 800, 'height': 800, 'annotations': [{'id': 15452, 'image_id': 5140, 'category_id': 1, 'segmentation': [[43.0, 747.0, 42.0, 748.0, 41.0, 748.0, 40.0, 749.0, 39.0, 750.0, 38.0, 751.0, 38.0, 752.0, 39.0, 753.0, 39.0, 754.0, 39.0, 755.0, 40.0, 756.0, 40.0, 757.0, 40.0, 758.0, 41.0, 759.0, 42.0, 760.0, 42.0, 761.0, 43.0, 762.0, 43.0, 763.0, 44.0, 764.0, 44.0, 765.0, 44.0, 766.0, 45.0, 767.0, 45.0, 768.0, 45.0, 769.0, 45.0, 770.0, 46.0, 771.0, 46.0, 772.0, 46.0, 773.0, 47.0, 774.0, 47.0, 775.0, 47.0, 776.0, 48.0, 777.0, 48.0, 778.0, 48.0, 779.0, 48.0, 780.0, 49.0, 781.0, 49.0, 782.0, 49.0, 783.0, 50.0, 784.0, 50.0, 785.0, 50.0, 786.0, 51.0, 787.0, 51.0, 788.0, 51.0, 789.0, 52.0, 790.0, 52.0, 791.0, 52.0, 792.0, 53.0, 793.0, 53.0, 794.0, 54.0, 795.0, 55.0, 796.0, 56.0, 797.0, 57.0, 798.0, 58.0, 798.0, 59.0, 798.0, 60.0, 798.0, 61.0, 798.0, 62.0, 798.0, 63.0, 798.0, 64.0, 797.0, 65.0, 797.0, 66.0, 796.0, 67.0, 796.0, 68.0, 795.0, 68.0, 794.0, 68.0, 793.0, 69.0, 792.0, 69.0, 791.0, 69.0, 790.0, 69.0, 789.0, 69.0, 788.0, 68.0, 787.0, 68.0, 786.0, 67.0, 785.0, 67.0, 784.0, 66.0, 783.0, 66.0, 782.0, 65.0, 781.0, 65.0, 780.0, 64.0, 779.0, 64.0, 778.0, 63.0, 777.0, 63.0, 776.0, 63.0, 775.0, 62.0, 774.0, 62.0, 773.0, 61.0, 772.0, 61.0, 771.0, 60.0, 770.0, 60.0, 769.0, 59.0, 768.0, 59.0, 767.0, 58.0, 766.0, 58.0, 765.0, 57.0, 764.0, 57.0, 763.0, 57.0, 762.0, 56.0, 761.0, 56.0, 760.0, 55.0, 759.0, 55.0, 758.0, 54.0, 757.0, 54.0, 756.0, 53.0, 755.0, 53.0, 754.0, 52.0, 753.0, 52.0, 752.0, 51.0, 751.0, 51.0, 750.0, 50.0, 749.0, 49.0, 748.0, 48.0, 748.0, 47.0, 748.0, 46.0, 747.0, 45.0, 747.0, 44.0, 747.0]], 'area': 799.0, 'bbox': [38.0, 747.0, 32.0, 52.0], 'iscrowd': 0}, {'id': 15453, 'image_id': 5140, 'category_id': 1, 'segmentation': [[38.0, 758.0, 37.0, 759.0, 36.0, 759.0, 35.0, 760.0, 35.0, 761.0, 34.0, 762.0, 34.0, 763.0, 34.0, 764.0, 34.0, 765.0, 34.0, 766.0, 35.0, 767.0, 35.0, 768.0, 35.0, 769.0, 35.0, 770.0, 34.0, 771.0, 33.0, 772.0, 32.0, 772.0, 31.0, 773.0, 30.0, 774.0, 29.0, 775.0, 29.0, 776.0, 29.0, 777.0, 30.0, 778.0, 30.0, 779.0, 30.0, 780.0, 30.0, 781.0, 31.0, 782.0, 31.0, 783.0, 32.0, 783.0, 33.0, 784.0, 34.0, 784.0, 35.0, 785.0, 36.0, 785.0, 37.0, 786.0, 38.0, 787.0, 39.0, 788.0, 40.0, 789.0, 41.0, 789.0, 42.0, 790.0, 43.0, 790.0, 44.0, 790.0, 45.0, 790.0, 46.0, 790.0, 47.0, 789.0, 48.0, 788.0, 48.0, 787.0, 49.0, 786.0, 49.0, 785.0, 49.0, 784.0, 48.0, 783.0, 48.0, 782.0, 48.0, 781.0, 47.0, 780.0, 47.0, 779.0, 47.0, 778.0, 47.0, 777.0, 46.0, 776.0, 46.0, 775.0, 46.0, 774.0, 45.0, 773.0, 45.0, 772.0, 45.0, 771.0, 44.0, 770.0, 44.0, 769.0, 44.0, 768.0, 44.0, 767.0, 43.0, 766.0, 43.0, 765.0, 43.0, 764.0, 42.0, 763.0, 42.0, 762.0, 41.0, 761.0, 41.0, 760.0, 40.0, 759.0, 39.0, 758.0]], 'area': 410.0, 'bbox': [29.0, 758.0, 21.0, 33.0], 'iscrowd': 0}]}\n"]},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-115-c1112cd25aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvisualize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_top_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-111-69de944e052b>\u001b[0m in \u001b[0;36mload_mask\u001b[0;34m(self, image_id)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 m = self.annToMask(annotation, image_info[\"height\"],\n\u001b[0;32m--> 156\u001b[0;31m                                    image_info[\"width\"])\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# Some objects are so small that they're less than 1 pixel area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;31m# and end up rounded out. Skip those objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-111-69de944e052b>\u001b[0m in \u001b[0;36mannToMask\u001b[0;34m(self, ann, height, width)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \"\"\"\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mrle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannToRLE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-111-69de944e052b>\u001b[0m in \u001b[0;36mannToRLE\u001b[0;34m(self, ann, height, width)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# polygon -- a single object might consist of multiple parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# we merge all parts into one mask rle code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mrles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrPyObjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mrle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'counts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'maskUtils' is not defined"]}],"source":["# Load and display random samples\n","image_ids = np.random.choice(dataset_train.image_ids, 4)\n","for image_id in image_ids:\n","    image, image_info = dataset_train.load_image(image_id)\n","    print(image_info['file_name'])\n","    mask, class_ids = dataset_train.load_mask(image_id)\n","    visualize.display_top_masks(image_info, image, mask, class_ids, dataset_train.class_names, limit=1)"]},{"cell_type":"markdown","metadata":{"id":"CyEQKAnMSZIi"},"source":["## Create Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1652205159146,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"7GCYogpmSZIj"},"outputs":[],"source":["# Create model in training mode\n","model = modellib.MaskRCNN(mode=\"training\", config=config,\n","                          model_dir=MODEL_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1652205159147,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"HjMmZKVaEqhJ"},"outputs":[],"source":["# you need to wrap the loss function with an empty lamda, when adding to the model\n","# model.keras_model.metrics_tensors = [] # \n","\n","\n","# # Tensorflow board\n","# logdir = os.path.join(\n","#     \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","# # load weights (mscoco) and exclude the output layers\n","# model.load_weights('mask_rcnn_coco.h5',\n","#                    by_name=True,\n","#                    exclude=[\n","#                        \"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\",\n","#                        \"mrcnn_mask\"\n","#                    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1652205159147,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"wUuC-fI4SZIl"},"outputs":[],"source":["# Which weights to start with?\n","init_with = \"coco\"  # imagenet, coco, or last\n","\n","if init_with == \"imagenet\":\n","    model.load_weights(model.get_imagenet_weights(), by_name=True)\n","elif init_with == \"coco\":\n","    # Load weights trained on MS COCO, but skip layers that\n","    # are different due to the different number of classes\n","    # See README for instructions to download the COCO weights\n","    model.load_weights(COCO_MODEL_PATH, by_name=True,\n","                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n","                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n","elif init_with == \"last\":\n","    # Load the last model you trained and continue training\n","    model.load_weights(model.find_last()[1], by_name=True)"]},{"cell_type":"markdown","metadata":{"id":"WzZuQxylSZIo"},"source":["## Training\n","\n","Train in two stages:\n","1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n","\n","2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1652205159147,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"j2jmfqdLSZIp"},"outputs":[],"source":["# Train the head branches\n","# Passing layers=\"heads\" freezes all layers except the head\n","# layers. You can also pass a regular expression to select\n","# which layers to train by name pattern.\n","\n","model.train(dataset_train, dataset_val,\n","            learning_rate=config.LEARNING_RATE, \n","            epochs=1, \n","            layers='heads')\n","\n","\n","\n","# checkpoint_filepath = '/tmp/checkpoint'\n","# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","#     filepath=checkpoint_filepath,\n","#     save_weights_only=False,\n","#     monitor='val_mrcnn_mask_loss',\n","#     mode='min',\n","#     save_best_only=True)\n","\n","# model.train(dataset_train, dataset_val,\n","#             learning_rate=config.LEARNING_RATE, \n","#             epochs=1, \n","#             layers='heads',\n","#             custom_callbacks=[model_checkpoint_callback])\n","\n","\n","\n","# mean_average_precision_callback = modellib.MeanAveragePrecisionCallback(model,\n","# model, dataset_val, calculate_map_at_every_X_epoch=3, verbose=1)\n","\n","# model.train(dataset_train, dataset_val,\n","#             learning_rate=config.LEARNING_RATE,\n","#             epochs=100,\n","#             layers='heads')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1652205159147,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"7wN1l921SZIt"},"outputs":[],"source":["# Fine tune all layers\n","# Passing layers=\"all\" trains all layers. You can also \n","# pass a regular expression to select which layers to\n","# train by name pattern.\n","\"\"\"model.train(dataset_train, dataset_val, \n","            learning_rate=config.LEARNING_RATE / 10,\n","            epochs=2, \n","            layers=\"all\")\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1652205159147,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"HRK53lnTSZIx"},"outputs":[],"source":["# Save weights\n","# Typically not needed because callbacks save after every epoch\n","# Uncomment to save manually\n","model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes3.h5\")\n","model.keras_model.save_weights(model_path)"]},{"cell_type":"markdown","metadata":{"id":"ldulTQXRSZI0"},"source":["## Detection"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1652205159148,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"LWhUidKESZI1"},"outputs":[],"source":["class InferenceConfig(ShapesConfig):\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","\n","inference_config = InferenceConfig()\n","\n","# Recreate the model in inference mode\n","model = modellib.MaskRCNN(mode=\"inference\", \n","                          config=inference_config,\n","                          model_dir=MODEL_DIR)\n","\n","# Get path to saved weights\n","# Either set a specific path or find last trained weights\n","model_path = os.path.join(ROOT_DIR, \"logs/mask_rcnn_shapes3.h5\")\n","# model_path = model.find_last()[1]\n","\n","# Load trained weights (fill in path to trained weights here)\n","assert model_path != \"\", \"Provide path to trained weights\"\n","print(\"Loading weights from \", model_path)\n","model.load_weights(model_path, by_name=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1652205159148,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"Y-aJVKAhSZI3"},"outputs":[],"source":["# Test on a random image\n","image_id = random.choice(dataset_val.image_ids)\n","original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","    modellib.load_image_gt(dataset_val, inference_config, \n","                           image_id)\n","\n","log(\"original_image\", original_image)\n","log(\"image_meta\", image_meta)\n","log(\"gt_class_id\", gt_class_id)\n","log(\"gt_bbox\", gt_bbox)\n","log(\"gt_mask\", gt_mask)\n","\n","# Remove alpha channel, if it has one\n","if original_image.shape[-1] == 4:\n","    original_image = original_image[..., :3]\n","\n","# visualize.display_images([original_image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 7))])\n","\n","visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n","                            dataset_train.class_names, figsize=(8, 8))\n","\n","\n","\n","# # image_ids = np.random.choice(dataset_val.image_ids,3)\n","# for image_id in dataset_val.image_ids:\n","# # for image_id in image_ids:\n","#     original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","#         modellib.load_image_gt(dataset_val, inference_config, \n","#                                image_id)\n","\n","#     log(\"original_image\", original_image)\n","#     log(\"image_meta\", image_meta)\n","#     log(\"gt_class_id\", gt_bbox)\n","#     log(\"gt_bbox\", gt_bbox)\n","#     log(\"gt_mask\", gt_mask)\n","\n","#     visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, dataset_train.class_names, figsize=(8, 8))\n","\n","#     # results = model.detect([original_image], verbose=1)\n","\n","#     # r = results[0]\n","#     # visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n","#     #                             dataset_val.class_names, r['scores'], ax=get_ax())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1652205159148,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"WwDFD2ghKez_"},"outputs":[],"source":["# Test on a random image\n","image_id = random.choice(dataset_val.image_ids)\n","original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","    modellib.load_image_gt(dataset_val, inference_config, \n","                           image_id, augmentation=False)\n","\n","log(\"original_image\", original_image)\n","log(\"image_meta\", image_meta)\n","log(\"gt_class_id\", gt_class_id)\n","log(\"gt_bbox\", gt_bbox)\n","log(\"gt_mask\", gt_mask)\n","\n","# Remove alpha channel, if it has one\n","if original_image.shape[-1] == 4:\n","    original_image = original_image[..., :3]\n","\n","visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n","                            dataset_train.class_names, figsize=(8, 8))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1652205159148,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"29ZwxYzNKh0i"},"outputs":[],"source":["# Test on a random image\n","image_id = random.choice(dataset_val.image_ids)\n","original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","    modellib.load_image_gt(dataset_val, inference_config, \n","                           image_id, augmentation=False)\n","\n","log(\"original_image\", original_image)\n","log(\"image_meta\", image_meta)\n","log(\"gt_class_id\", gt_class_id)\n","log(\"gt_bbox\", gt_bbox)\n","log(\"gt_mask\", gt_mask)\n","\n","# Remove alpha channel, if it has one\n","if original_image.shape[-1] == 4:\n","    original_image = original_image[..., :3]\n","\n","visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n","                            dataset_train.class_names, figsize=(8, 8))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1652205159148,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"TkpMYZzso71f"},"outputs":[],"source":["# Test on a random image\n","image_id = random.choice(dataset_val.image_ids)\n","original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","    modellib.load_image_gt(dataset_val, inference_config, \n","                           image_id)\n","\n","log(\"original_image\", original_image)\n","log(\"image_meta\", image_meta)\n","log(\"gt_class_id\", gt_class_id)\n","log(\"gt_bbox\", gt_bbox)\n","log(\"gt_mask\", gt_mask)\n","\n","visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n","                            dataset_train.class_names, figsize=(8, 8))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1652205159148,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"xj0dQI-Lo87X"},"outputs":[],"source":["results = model.detect([original_image], verbose=1)\n","\n","r = results[0]\n","visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n","                            dataset_val.class_names, r['scores'], ax=get_ax())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1652205159148,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"-s8KGn9KpBhc"},"outputs":[],"source":["# Compute VOC-Style mAP @ IoU=0.5\n","# Running on 30 images. Increase for better accuracy.\n","image_ids = np.random.choice(dataset_val.image_ids, 10) # 30\n","APs = []\n","for image_id in image_ids:\n","    # Load image and ground truth data\n","    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","        modellib.load_image_gt(dataset_val, inference_config,\n","                               image_id)\n","    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 1)\n","    # Run object detection\n","    results = model.detect([image], verbose=1)\n","    r = results[0]\n","    # Compute AP\n","    AP, precisions, recalls, overlaps =\\\n","        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n","                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n","    APs.append(AP)\n","    \n","print(\"mAP: \", np.mean(APs))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1652205159149,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"kaGHERqbpF95"},"outputs":[],"source":["X_test = np.zeros((len(test_ids), config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], 3), dtype=np.uint8)\n","sizes_test = []\n","_test_ids = []\n","\n","print('Getting and resizing test images ... ')\n","#sys.stdout.flush()\n","for n, id_ in enumerate(test_ids):\n","    _test_ids.append([id_])\n","    path = TEST_PATH + id_\n","    img = imread(path + '/images/' + id_ + '.png')[:,:,:3]\n","    sizes_test.append([img.shape[0], img.shape[1]])\n","    img = resize(img, (config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1]), mode='constant', preserve_range=True)\n","    X_test[n] = img"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1652205159149,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"qalaG3UepH4k"},"outputs":[],"source":["print(\"checking a test image with masks ...\")\n","results = model.detect([X_test[7]], verbose=1)\n","\n","r = results[0]\n","visualize.display_instances(X_test[7], r['rois'], r['masks'], r['class_ids'], \n","                            dataset_val.class_names, r['scores'], ax=get_ax())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1652205159149,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"ZyAT1glESZI7"},"outputs":[],"source":["\n","# # load photograph\n","# img = load_img('ship.jpg')\n","# img = img_to_array(img)\n","\n","results = model.detect([original_image], verbose=1)\n","\n","r = results[0]\n","visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n","                            dataset_val.class_names, r['scores'], ax=get_ax())"]},{"cell_type":"markdown","metadata":{"id":"i8mN9BsDSZI-"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1652205159149,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"Ib5pJXg3SZJA"},"outputs":[],"source":["# Compute VOC-Style mAP @ IoU=0.5\n","# Running on 30 images. Increase for better accuracy.\n","image_ids = np.random.choice(dataset_val.image_ids, 10)\n","APs = []\n","print(image_ids)\n","for image_id in image_ids:\n","    # Load image and ground truth data\n","    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","        modellib.load_image_gt(dataset_val, inference_config,\n","                               image_id)\n","    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n","    # Run object detection\n","    results = model.detect([image], verbose=0)\n","    r = results[0]\n","    # Compute AP\n","    AP, precisions, recalls, overlaps =\\\n","        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n","                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n","    APs.append(AP)\n","print(APs)\n","    \n","print(\"mAP: \", np.mean(APs))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1652205159149,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"OMsT8341JwgU"},"outputs":[],"source":["\n","X_test = np.zeros((len(test_ids), config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], 3), dtype=np.uint8)\n","sizes_test = []\n","_test_ids = []\n","\n","print('Getting and resizing test images ... ')\n","#sys.stdout.flush()\n","for n, id_ in enumerate(test_ids):\n","    _test_ids.append([id_])\n","    path = TEST_PATH + id_\n","    img = imread(path + '/images/' + id_ + '.png')[:,:,:3]\n","    sizes_test.append([img.shape[0], img.shape[1]])\n","    img = resize(img, (config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1]), mode='constant', preserve_range=True)\n","    X_test[n] = img"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1652205159149,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"6oyfiLFaSZJC"},"outputs":[],"source":["print(\"checking a test image with masks ...\")\n","results = model.detect([X_test[7]], verbose=1)\n","\n","r = results[0]\n","visualize.display_instances(X_test[7], r['rois'], r['masks'], r['class_ids'], \n","                            dataset_val.class_names, r['scores'], ax=get_ax())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1652205159149,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"9zmpPOZFZ6ad"},"outputs":[],"source":["# Run-length encoding for submission\n","def rle_encoding(x):\n","    dots = np.where(x.T.flatten() == 1)[0]\n","    run_lengths = []\n","    prev = -2\n","    for b in dots:\n","        if (b>prev+1): run_lengths.extend((b + 1, 0))\n","        run_lengths[-1] += 1\n","        prev = b\n","    return run_lengths\n","  \n","  \n","def prob_to_rles(x):\n","   \n","    for i in range(0, len(x[1, 1, :])):\n","        y = x[:, :, i]\n","        yield rle_encoding(np.squeeze(y))\n","\n","new_test_ids = []\n","rles = []\n","\n","print(len(test_ids))\n","\n","for n in range(0, len(test_ids)):\n","\n","    results = model.detect([X_test[n]], verbose=0)\n","     \n","    r = results[0]\n","\n","    number_of_masks = len(r['masks'][1, 1, :])\n","    \n","    pred_masks = np.zeros([sizes_test[n][0], sizes_test[n][1], number_of_masks], dtype=np.uint8)\n","   \n","    for m in range(0, number_of_masks):\n","        pred_mask_temp = r['masks'][:, :, m]\n","        pred_masks[:, :, m] = resize(np.squeeze(pred_mask_temp), (sizes_test[n][0], sizes_test[n][1]), mode='constant', preserve_range=True)\n","        \n","    # Handle occlusions\n","    occlusion = np.logical_not(pred_masks[:, :, -1]).astype(np.uint8)\n","    for i in range(number_of_masks-2, -1, -1):\n","        pred_masks[:, :, i] = pred_masks[:, :, i] * occlusion\n","        occlusion = np.logical_and(occlusion, np.logical_not(pred_masks[:, :, i]))\n","        \n","    rle = list(prob_to_rles(pred_masks))\n","        \n","            \n","    em = 0\n","    while (em < len(rle)):\n","          if len(rle[em]) == 1:\n","              print (\"mask has one pixel\")\n","          if len(rle[em]) == 0:\n","              print (\"mask is empty. deleting mask\")\n","              del rle[em]\n","          else:\n","              em = em + 1\n","    \n","    \n","    k = 0\n","    \n","    for m in range(0, len(rle)):\n","        while (k < len(rle[m]) ):\n","            if rle[m][k] + rle[m][k+1] >= sizes_test[n][0] * sizes_test[n][1]:\n","                print(\"Index was outside the bounds of the array.    index = \", rle[m][k] + rle[m][k+1], \"    array size = \", sizes_test[n][0] * sizes_test[n][1] )\n","                rle[m][k+1] = sizes_test[n][0] * sizes_test[n][1] - rle[m][k] - 1\n","                #print(\"run length fixed \")\n","                print(\"run length fixed.    index = \", rle[m][k] + rle[m][k+1], \"    array size = \", sizes_test[n][0] * sizes_test[n][1] )\n","                if rle[m][k+1] < 0:\n","                    print(\"run lngth is negative. deleting the index and run length   \")\n","                    del rle[m][k]\n","                    del rle[m][k]\n","                if rle[m][k] < 0:\n","                    print(\"index is negative. deleting the index and run length   \")\n","                    del rle[m][k]\n","                    del rle[m][k]\n","                    \n","           \n","                    \n","                 \n","                    \n","                #check that pixels are ordered \n","                if rle[m][k] > rle[m][k+2]:\n","                   print (\"pixel not ordered\")\n","                \n","                #check that a pixel is not duplicated\n","                if rle[m][k] + rle[m][k+1] >= rle[m][k+2]:\n","                   print (\"pixel duplicated\")\n","                \n","            k = k + 2\n","    k = 0           \n","    for m in range(0, len(rle)):\n","        \n","        while (k < len(rle[m]) ):\n","            if rle[m][k] + rle[m][k+1] >= sizes_test[n][0] * sizes_test[n][1]:\n","                print(\"Index was outside the bounds of the array.    index = \", rle[m][k] + rle[m][k+1], \"    array size = \", sizes_test[n][0] * sizes_test[n][1] )\n","            k = k + 2   \n","    \n","    rles.extend(rle)\n","    new_test_ids.extend(_test_ids[n] * len(rle))\n","\n","\n","# Create submission DataFrame\n","\n","sub = pd.DataFrame()\n","sub['ImageId'] = new_test_ids\n","sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n","sub.to_csv('sub-dsbowl2018.csv', index=False)\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1652205159150,"user":{"displayName":"Will Harris","userId":"10945786761760727903"},"user_tz":-60},"id":"eRiDaqhmnsJf"},"outputs":[],"source":["files.download('sub-dsbowl2018.csv')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"HRSID.ipynb","provenance":[]},"interpreter":{"hash":"320d996fe699cd062c0bc0441649ebf56930ded80e61eaa1b60f584677fcf0e9"},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
